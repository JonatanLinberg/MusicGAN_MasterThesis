{"cells":[{"cell_type":"markdown","source":["### Open in [Google Colab](https://colab.research.google.com/drive/12jQXxqLUIfRZmdsAFX5ENLr2evztxIZO?usp=sharing)"],"metadata":{"id":"LYGX9xZDp0d2"},"id":"LYGX9xZDp0d2"},{"cell_type":"markdown","id":"1c434783","metadata":{"id":"1c434783"},"source":["# Setup"]},{"cell_type":"code","execution_count":1,"id":"Z4zLWqbNAETb","metadata":{"id":"Z4zLWqbNAETb","executionInfo":{"status":"ok","timestamp":1683187482718,"user_tz":-120,"elapsed":519,"user":{"displayName":"Yupeng Li","userId":"16359597207707359145"}}},"outputs":[],"source":["%load_ext tensorboard"]},{"cell_type":"code","execution_count":null,"id":"4yuR4I1kwM9d","metadata":{"id":"4yuR4I1kwM9d","colab":{"base_uri":"https://localhost:8080/"},"outputId":"4887c0d8-7143-4172-8e2e-91ca0f8f2150"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["try:\n","  from google.colab import drive\n","  drive.mount('/content/drive', force_remount=True)\n","  path_top = '/content/drive/MyDrive/Master Thesis/repo'\n","  !cp \"$path_top/dataset.zip\" .\n","  !unzip -q dataset.zip\n","  !pip install ltn -q\n","  !pip install tensorflow-io -q\n","  !pip install tensorflow-addons -q\n","  !cp \"$path_top/code/params.py\" .\n","  !cp \"$path_top/code/melspec.py\" .\n","except Exception:\n","  path_top = ''"]},{"cell_type":"code","execution_count":null,"id":"3c62efb1","metadata":{"id":"3c62efb1"},"outputs":[],"source":["import os\n","import time\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","import tensorflow_io as tfio\n","import tensorflow_addons as tfaddons\n","import datetime\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","import tensorflow_datasets as tfds\n","import IPython\n","from tensorflow.keras import layers\n","from scipy.io import wavfile as wf\n","from scipy.stats import norm\n","\n","import params\n","from melspec import MelSpec"]},{"cell_type":"code","execution_count":null,"id":"908775f2","metadata":{"id":"908775f2"},"outputs":[],"source":["LABEL_RATE  = 2  # rate of labels in Hz\n","\n","audio_path  = 'dataset/audio'\n","label_path  = 'dataset/DEAM_Annotations/annotations/annotations averaged per song/dynamic (per second annotations)'"]},{"cell_type":"markdown","id":"798201aa","metadata":{"id":"798201aa"},"source":["---\n","# Dataset"]},{"cell_type":"markdown","id":"3bbdd913","metadata":{"id":"3bbdd913"},"source":["## Load Audio Data"]},{"cell_type":"code","execution_count":null,"id":"48d16576","metadata":{"id":"48d16576"},"outputs":[],"source":["def load_audio_data():\n","    ds_audio_train = tf.keras.utils.audio_dataset_from_directory(\n","        audio_path,\n","        labels=None,\n","        batch_size=None,\n","        shuffle=False,\n","        seed=None,\n","        output_sequence_length=(45 * params.SAMPLE_RATE), # 45 seconds\n","        follow_links=True\n","    )\n","\n","    return ds_audio_train"]},{"cell_type":"markdown","id":"51c7500d","metadata":{"id":"51c7500d"},"source":["## Audio Preprocessing"]},{"cell_type":"markdown","id":"3ac8a71a","metadata":{"id":"3ac8a71a"},"source":["### Skip first 15 seconds\n","The first 15 seconds are not annotated. Thus, if labels are used, the first 15 seconds of audio should be skipped."]},{"cell_type":"code","execution_count":null,"id":"899f8a17","metadata":{"id":"899f8a17"},"outputs":[],"source":["_15_SEC_SKIP = 15 * params.SAMPLE_RATE\n","\n","def skip15sec(data):\n","    return data[_15_SEC_SKIP:]"]},{"cell_type":"markdown","id":"f6b35a5b","metadata":{"id":"f6b35a5b"},"source":["### Make mono"]},{"cell_type":"code","execution_count":null,"id":"977dde67","metadata":{"id":"977dde67"},"outputs":[],"source":["def to_mono(data):\n","    return data[...,0] # take first channel"]},{"cell_type":"markdown","id":"16b44fbf","metadata":{"id":"16b44fbf"},"source":["### Reshape data to match labels"]},{"cell_type":"code","execution_count":null,"id":"f56e644e","metadata":{"id":"f56e644e"},"outputs":[],"source":["def reshape_data(data):\n","    if (len(data) == 0):\n","        return data\n","    #n_channels = 2\n","    return tf.reshape(data, (LABEL_RATE * 30, params.SAMPLE_RATE//LABEL_RATE, ))#n_channels))"]},{"cell_type":"code","execution_count":null,"id":"H9pd-MrAzTWY","metadata":{"id":"H9pd-MrAzTWY"},"outputs":[],"source":["# cuts a full-song spectrogram into pieces according to the input size of the model\n","def sequence_spect(data, seq_len=params.SEQ_LENGTH, lstm_window=params.LSTM_WINDOW, bins=params.MEL_BINS):\n","    if len(data) == 0:\n","        return data\n","    trim = len(data) - seq_len * lstm_window\n","    i = np.random.randint(trim+1)\n","    trim = trim - i\n","    data = data[i:-trim if trim else None]\n","    if (len(data) != (seq_len * lstm_window)):\n","        raise Exception(\"Uh oh\")\n","\n","#    data = data[:seq_len * lstm_window]\n","\n","    return tf.reshape(data, (seq_len, lstm_window, bins))"]},{"cell_type":"code","execution_count":null,"id":"KNKxqkIalEHS","metadata":{"id":"KNKxqkIalEHS"},"outputs":[],"source":["# changes (batch_size, seq_length) into (batch_size * seq_length,)\n","def unsequence_spect(data, spec_shape = (params.LSTM_WINDOW, params.MEL_BINS)):\n","    return tf.reshape(data, (-1, *spec_shape))"]},{"cell_type":"markdown","id":"53b80ac6","metadata":{"id":"53b80ac6"},"source":["### Mel-scaled Fourier transform"]},{"cell_type":"code","execution_count":null,"id":"06090f08","metadata":{"id":"06090f08"},"outputs":[],"source":["# create melspec layer for audio-to-spectrogram transformation\n","mel_spec = MelSpec(\n","    frame_length = params.STFT_WINDOW,\n","    frame_step = params.STFT_HOP,\n","    sampling_rate = params.SAMPLE_RATE,\n","    num_mel_channels = params.MEL_BINS,\n","    freq_min = params.MEL_FREQ_MIN,\n","    freq_max = params.MEL_FREQ_MAX,\n","    min_db = -params.DB_MIN,\n",")\n","\n","# wrapper function for melspec layer\n","def mel_spectrogram(data):\n","    S = mel_spec(tf.expand_dims(data, -1))\n","    return S"]},{"cell_type":"markdown","source":["### Audio Reconstruction (Griffin-Lim)"],"metadata":{"id":"gbHC67KDl1F4"},"id":"gbHC67KDl1F4"},{"cell_type":"code","source":["# Implements Griffin-Lim for audio reconstruction\n","def mel_to_audio(S):\n","    S = tf.cast(S, tf.float32)\n","\n","    # mel to linear\n","    mel_matrix = tf.signal.linear_to_mel_weight_matrix(\n","        num_mel_bins=params.MEL_BINS,\n","        num_spectrogram_bins=params.STFT_WINDOW//2+1,\n","        sample_rate=params.SAMPLE_RATE,\n","        lower_edge_hertz=params.MEL_FREQ_MIN,\n","        upper_edge_hertz=params.MEL_FREQ_MAX,\n","        dtype=tf.dtypes.float32\n","    )\n","    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n","        mel_inversion_matrix = tf.constant(\n","            np.nan_to_num(\n","                np.divide(mel_matrix.numpy().T, np.sum(mel_matrix.numpy(), axis=1))\n","            ).T\n","        )\n","    S = tf.tensordot(S, tf.transpose(mel_inversion_matrix), 1)\n","    \n","    \n","    # dB to amplitude\n","    S = tf.pow(tf.ones(tf.shape(S)) * 10.0, (S) / 20)  # 10^(dB / 20)\n","    \n","    # Griffin-Lim:\n","    return tfio.audio.inverse_spectrogram(S, params.STFT_WINDOW, params.STFT_WINDOW, params.STFT_HOP, iterations=params.GRIFFIN_LIM_ITER).numpy()"],"metadata":{"id":"W6F73-2elmE_"},"id":"W6F73-2elmE_","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"57ca1b10","metadata":{"id":"57ca1b10"},"source":["### Map preprocessing to dataset"]},{"cell_type":"code","execution_count":null,"id":"9bf2a920","metadata":{"id":"9bf2a920"},"outputs":[],"source":["ds_audio_train = load_audio_data()\n","\n","ds_audio_train = ds_audio_train.map(skip15sec)\n","ds_audio_train = ds_audio_train.map(to_mono)\n","ds_spect_train = ds_audio_train.map(mel_spectrogram)\n","ds_spect_train = ds_spect_train.map(sequence_spect)\n","#ds_spect_train = ds_spect_train.map(unsequence_spect)\n","ds_spect_train = ds_spect_train.unbatch()"]},{"cell_type":"markdown","id":"37067b4d","metadata":{"id":"37067b4d"},"source":["### Check audio data"]},{"cell_type":"code","execution_count":null,"id":"VCjqa8SLxOKi","metadata":{"id":"VCjqa8SLxOKi"},"outputs":[],"source":["## S := TxF matrix\n","def show_spectrogram(S, title='', n_spect=1, scale=5, rhythm_plots=True):\n","    #fig, ax = plt.subplots(ncols=1, figsize=(15,4))\n","    #cax = ax.matshow(S.T, aspect='auto', origin='lower')\n","    #fig.colorbar(cax)\n","    plt.figure(figsize=(scale*n_spect, scale))\n","    plt.imshow(S.T, origin='lower', vmin=-40, vmax=70)\n","    plt.axis('off')\n","    plt.title(title)\n","    plt.show()\n","    plt.close()\n","    \n","    if rhythm_plots:\n","        x = tf.reduce_mean(S, axis=1, keepdims=True)\n","        fig = plt.figure(figsize=(7, 2))\n","        ax = fig.add_subplot(1, 2, 1)\n","        ax.plot(x)\n","        \n","        y = tf.abs(tf.signal.stft(\n","            tf.squeeze(x),\n","            x.shape[0],\n","            1,\n","            window_fn=None\n","        ))\n","        y = y[:, params.RL_LOW_BOUND:params.RL_HI_BOUND]\n","        y = tf.reshape(y, (y.shape[1], y.shape[0]))\n","        ax = fig.add_subplot(1, 2, 2)\n","        ax.plot(y)\n","        plt.show()\n","        plt.close()\n","\n","def show_audio(S):\n","    S = tf.squeeze(S)\n","    rec_aud = mel_to_audio(S)\n","    wf.write('rec.wav', params.SAMPLE_RATE, rec_aud)\n","    rec = IPython.display.Audio('rec.wav')\n","    display(rec)"]},{"cell_type":"code","execution_count":null,"id":"f0b84dcc","metadata":{"id":"f0b84dcc","scrolled":false},"outputs":[],"source":["# FOR SPECTROGRAMS\n","skip = 0\n","n = 1\n","for x in ds_spect_train.skip(skip).take(1):\n","    x = x.numpy()\n","    print(np.min(x), np.max(x))\n","    print(x.shape)\n","    show_spectrogram(x)\n","    print(\"Reconstructed Audio:\")\n","    show_audio(x)\n","\n","_=\"\"\"\n","# AUDIO for reference\n","aud = None\n","for a in ds_audio_train.skip(skip).take(n):\n","    if aud is None:\n","       aud = a.numpy()\n","    else:\n","        aud = np.concatenate((aud, a), axis=0)\n","\n","wf.write('test.wav', params.SAMPLE_RATE, aud)\n","test = Audio('test.wav')\n","display(test)\n","\"\"\""]},{"cell_type":"markdown","id":"9c6fa0e2","metadata":{"id":"9c6fa0e2"},"source":["## Load Labels"]},{"cell_type":"code","execution_count":null,"id":"86f409e8","metadata":{"id":"86f409e8"},"outputs":[],"source":["ds_valence = pd.read_csv(f\"{label_path}/valence.csv\")\n","ds_valence = ds_valence.dropna(axis=1)\n","ds_valence = ds_valence.drop(columns='song_id')\n","#ds_valence"]},{"cell_type":"code","execution_count":null,"id":"7777d1a1","metadata":{"id":"7777d1a1"},"outputs":[],"source":["ds_arousal = pd.read_csv(f\"{label_path}/arousal.csv\")\n","ds_arousal = ds_arousal.dropna(axis=1)\n","ds_arousal = ds_arousal.drop(columns='song_id')\n","#ds_arousal"]},{"cell_type":"markdown","source":["### average labels for each spectrogram"],"metadata":{"id":"KQBIH-E2k-Jo"},"id":"KQBIH-E2k-Jo"},{"cell_type":"code","execution_count":null,"id":"yQeeq5iSboef","metadata":{"id":"yQeeq5iSboef"},"outputs":[],"source":["n_lab_per_spect = 6\n","\n","ds_arousal = ds_arousal.transpose()\n","ds_arousal = ds_arousal.groupby(np.arange(len(ds_arousal.index))//n_lab_per_spect).mean()\n","ds_arousal = ds_arousal.transpose()\n","ds_arousal = np.reshape(ds_arousal.to_numpy(), (len(ds_spect_train), 1))\n","\n","ds_valence = ds_valence.transpose()\n","ds_valence = ds_valence.groupby(np.arange(len(ds_valence.index))//n_lab_per_spect).mean()\n","ds_valence = ds_valence.transpose()\n","ds_valence = np.reshape(ds_valence.to_numpy(), (len(ds_spect_train), 1))"]},{"cell_type":"code","execution_count":null,"id":"6Nqzm7h5jszW","metadata":{"id":"6Nqzm7h5jszW"},"outputs":[],"source":["ARO_MEAN, ARO_STD = ds_arousal.mean(), ds_arousal.std()\n","\n","_ = plt.hist(ds_arousal, bins=100, density=True, range=[-1, 1])\n","plt.plot(np.arange(-1, 1, 0.01), norm.pdf(np.arange(-1, 1, 0.01), ARO_MEAN, ARO_STD))\n","plt.title(\"Arousal distribution\")\n","print(f\"Mean: {ARO_MEAN}\\nStddev: {ARO_STD}\")"]},{"cell_type":"code","execution_count":null,"id":"XQU-Ge_ZjuNN","metadata":{"id":"XQU-Ge_ZjuNN"},"outputs":[],"source":["VAL_MEAN, VAL_STD = ds_valence.mean(), ds_valence.std()\n","\n","_ = plt.hist(ds_valence, bins=100, density=True, range=[-1, 1])\n","plt.plot(np.arange(-1, 1, 0.01), norm.pdf(np.arange(-1, 1, 0.01), VAL_MEAN, VAL_STD))\n","plt.title(\"Valence distribution\")\n","print(f\"Mean: {VAL_MEAN}\\nStddev: {VAL_STD}\")"]},{"cell_type":"markdown","id":"c05a4f11","metadata":{"id":"c05a4f11"},"source":["## Combine audio and labels"]},{"cell_type":"code","execution_count":null,"id":"5ab1208f","metadata":{"id":"5ab1208f"},"outputs":[],"source":["ds_arousal = tf.data.Dataset.from_tensor_slices(ds_arousal.astype(np.float32))\n","ds_valence = tf.data.Dataset.from_tensor_slices(ds_valence.astype(np.float32))\n","\n","ds_train = tf.data.Dataset.zip((\n","    ds_spect_train,\n","    (ds_arousal, ds_valence)\n","))"]},{"cell_type":"code","execution_count":null,"id":"346c46c6","metadata":{"id":"346c46c6","scrolled":true},"outputs":[],"source":["ds_train = ds_train.shuffle(buffer_size=80*params.BATCH_SIZE, reshuffle_each_iteration=True)\n","ds_train = ds_train.batch(params.BATCH_SIZE)\n","ds_train = ds_train.prefetch(buffer_size=tf.data.AUTOTUNE)"]},{"cell_type":"markdown","id":"IVJzJbwEucwl","metadata":{"id":"IVJzJbwEucwl"},"source":["# Model"]},{"cell_type":"markdown","id":"ek9LPu5TAYMq","metadata":{"id":"ek9LPu5TAYMq"},"source":["## Generator\n"]},{"cell_type":"code","execution_count":null,"id":"sOHZsw_PAM4s","metadata":{"id":"sOHZsw_PAM4s"},"outputs":[],"source":["def create_generator(noise_dim = params.GEN_NOISE_DIM):\n","\n","    emb_size = params.LSTM_WINDOW//32 * params.MEL_BINS//32\n","    emb_ch = 16\n","    \n","    noise_inputs = tf.keras.Input(batch_shape=(None, noise_dim))\n","    arousal_inputs = tf.keras.Input(batch_shape=(None, 1))\n","    valence_inputs = tf.keras.Input(batch_shape=(None, 1))\n","    rhythm_inputs = tf.keras.Input(batch_shape=(None, params.RL_HI_BOUND-params.RL_LOW_BOUND))\n","\n","    # embed labels\n","    x_a = layers.Dense(emb_size * emb_ch, activation='relu')(arousal_inputs)\n","    x_v = layers.Dense(emb_size * emb_ch, activation='relu')(valence_inputs)\n","    x_r = layers.Dense(emb_size * emb_ch, activation='relu')(rhythm_inputs)\n","\n","    # noise\n","    x = layers.Dense(emb_size * emb_ch, activation='relu')(noise_inputs)\n","\n","    x = tf.stack([x, x_a, x_v, x_r], axis=-1)\n","\n","    # deconv upsample\n","    x = layers.Reshape((params.LSTM_WINDOW//32, params.MEL_BINS//32, emb_ch * 4))(x)\n","    for filters in (512, 512, 256, 256, 128):\n","        #x = layers.Conv2DTranspose(filters, (3,3), strides=2, padding='same')(x)\n","        x = layers.UpSampling2D()(x)\n","        x = layers.Conv2D(filters, (3, 3), strides=1, padding='same')(x)\n","        x = layers.ReLU()(x)\n","        x = layers.BatchNormalization()(x)\n","\n","    x = layers.Conv2D(1, (3,3), strides=1, padding='same', activation='sigmoid')(x)\n","\n","    spec = layers.Reshape((params.LSTM_WINDOW, params.MEL_BINS))(x)\n","\n","    outputs = spec\n","    return tf.keras.Model([noise_inputs, [arousal_inputs, valence_inputs], rhythm_inputs], outputs)\n","\n","#create_generator().summary()\n","#tf.keras.utils.plot_model(create_generator(), show_shapes=True)"]},{"cell_type":"markdown","id":"iv4lmldTqEde","metadata":{"id":"iv4lmldTqEde"},"source":["\n","## Discriminator"]},{"cell_type":"code","execution_count":null,"id":"VPXuwALZEL1U","metadata":{"id":"VPXuwALZEL1U"},"outputs":[],"source":["class WeightClip(tf.keras.constraints.Constraint):\n","    '''Clips the weights incident to each hidden unit to be inside a range\n","    '''\n","    def __init__(self, c=0.01, **kwargs):\n","        self.c = c\n","\n","    def __call__(self, p):\n","        return tf.keras.backend.clip(p, -self.c, self.c)\n","\n","def create_discriminator(\n","    input_shape = (params.LSTM_WINDOW, params.MEL_BINS)):\n","    \n","    inputs = tf.keras.Input(shape=input_shape)\n","    arousal_inputs = tf.keras.Input(batch_shape=(None, 1))\n","    valence_inputs = tf.keras.Input(batch_shape=(None, 1))\n","    \n","    n_emb_upsampling = 3\n","    emb_shape = (input_shape[0]//(2**n_emb_upsampling), input_shape[1]//(2**n_emb_upsampling))\n","\n","    x_a = layers.Dense(tf.reduce_prod(emb_shape), activation='relu', kernel_constraint=WeightClip(params.WGAN_D_CONSTRAINT))(arousal_inputs)\n","    x_v = layers.Dense(tf.reduce_prod(emb_shape), activation='relu', kernel_constraint=WeightClip(params.WGAN_D_CONSTRAINT))(valence_inputs)\n","\n","    x_a = layers.Reshape((*emb_shape, 1))(x_a)\n","    x_v = layers.Reshape((*emb_shape, 1))(x_v)\n","    \n","    for _ in range(n_emb_upsampling):\n","        x_a = layers.UpSampling2D()(x_a)\n","        x_v = layers.UpSampling2D()(x_v)\n","\n","    # downsample\n","    x = tf.expand_dims(inputs, axis=-1)\n","    x = tf.concat((x, x_a, x_v), axis=-1)\n","    for filters in (64, 64, 128, 128, 256, 256):\n","        res = layers.Conv2D(filters, 1, strides=2, padding='same', kernel_constraint=WeightClip(params.WGAN_D_CONSTRAINT))(x)\n","\n","        x = layers.Conv2D(filters, (3,3), strides=2, padding='same', kernel_constraint=WeightClip(params.WGAN_D_CONSTRAINT))(x)\n","        x = layers.ReLU()(x)\n","        x = layers.Dropout(0.3)(x)\n","        x = layers.BatchNormalization()(x)\n","\n","        x = layers.add([x, res])\n","\n","    x = layers.Flatten()(x)\n","    \"\"\"x = layers.LSTM(\n","        lstm_units, \n","        return_sequences=True, \n","        kernel_constraint=d_constraint, \n","    )(x)\"\"\"\n","    x = layers.Dense(1, activation='linear', kernel_constraint=WeightClip(params.WGAN_D_CONSTRAINT))(x)\n","\n","    return tf.keras.Model([inputs, arousal_inputs, valence_inputs], x)\n","\n","#create_discriminator().summary()\n","#tf.keras.utils.plot_model(create_discriminator(), show_shapes=True)"]},{"cell_type":"code","execution_count":null,"id":"z-r8RrxqLC84","metadata":{"id":"z-r8RrxqLC84"},"outputs":[],"source":["def create_patch_discriminator(\n","    input_shape = (params.PATCH_WIDTH, params.MEL_BINS),\n","    lstm_units = params.LSTM_UNITS_DISC):\n","    \n","    inputs = tf.keras.Input(shape=input_shape)\n","    arousal_inputs = tf.keras.Input(batch_shape=(None, 1))\n","    valence_inputs = tf.keras.Input(batch_shape=(None, 1))\n","    \n","    n_emb_upsampling = 1\n","    emb_shape = (input_shape[0]//(2**n_emb_upsampling), input_shape[1]//(2**n_emb_upsampling))\n","\n","    x_a = layers.Dense(tf.reduce_prod(emb_shape), activation='relu', kernel_constraint=WeightClip(params.WGAN_D_CONSTRAINT))(arousal_inputs)\n","    x_v = layers.Dense(tf.reduce_prod(emb_shape), activation='relu', kernel_constraint=WeightClip(params.WGAN_D_CONSTRAINT))(valence_inputs)\n","\n","    x_a = layers.Reshape((*emb_shape, 1))(x_a)\n","    x_v = layers.Reshape((*emb_shape, 1))(x_v)\n","    \n","    for _ in range(n_emb_upsampling):\n","        x_a = layers.UpSampling2D()(x_a)\n","        x_v = layers.UpSampling2D()(x_v)\n","\n","\n","    # downsample\n","    x = tf.expand_dims(inputs, axis=-1)\n","    x = tf.concat((x, x_a, x_v), axis=-1)\n","    last_filter = 1\n","    for filters in (64, 128, 128, 256):\n","        res = layers.Conv2D(filters, 1, strides=2, padding='same', kernel_constraint=WeightClip(params.WGAN_D_CONSTRAINT))(x)\n","\n","        x = layers.ZeroPadding2D(padding=(0, 1))(x)\n","        x = layers.Conv2D(filters, (2,4), strides=(2, 2), padding='valid', kernel_constraint=WeightClip(params.WGAN_D_CONSTRAINT))(x)\n","        x = layers.ReLU()(x)\n","        x = layers.Dropout(0.3)(x)\n","        x = layers.BatchNormalization()(x)\n","\n","        x = layers.add([x, res])\n","        last_filter = filters\n","\n","    x = layers.Flatten()(x)\n","    x = layers.Dense(1, activation='linear', kernel_constraint=WeightClip(params.WGAN_D_CONSTRAINT))(x)\n","\n","    return tf.keras.Model([inputs, arousal_inputs, valence_inputs], x)\n","\n","#create_patch_discriminator().summary()\n","#tf.keras.utils.plot_model(create_patch_discriminator(), show_shapes=True)"]},{"cell_type":"markdown","id":"p-5SPePwQ4Vo","metadata":{"id":"p-5SPePwQ4Vo"},"source":["# Training"]},{"cell_type":"markdown","id":"1C4OUW1gAshw","metadata":{"id":"1C4OUW1gAshw"},"source":["## misc functions"]},{"cell_type":"markdown","source":["### Spectrogram Normalisation layers\n","* Limits the data to dB between -40 and 70\n","* Rescales into the range [0, 1]"],"metadata":{"id":"RJSiETI8n2J-"},"id":"RJSiETI8n2J-"},{"cell_type":"code","source":["class NormSpect(layers.Layer):\n","    def __init__(self, min_dB=-40., max_dB=70., **kwargs):\n","        super().__init__(**kwargs)\n","        self.min = min_dB\n","        self.max = max_dB\n","        \n","        r = self.max - self.min\n","        self.rescale = layers.Rescaling(\n","            scale=1./r, \n","            offset=(-self.min)/r\n","        )\n","\n","    def call(self, spect):\n","        spect = tf.clip_by_value(spect, self.min, self.max)\n","        spect = self.rescale(spect)\n","        return spect\n","\n","\n","class DeNormSpect(layers.Layer):\n","    def __init__(self, min_dB=-40., max_dB=70., **kwargs):\n","        super().__init__(**kwargs)\n","        self.min = min_dB\n","        self.max = max_dB\n","\n","        r = self.max - self.min\n","        self.rescale = layers.Rescaling(\n","            scale=r,\n","            offset=self.min\n","        )\n","    \n","    def call(self, spect):\n","        spect = tf.clip_by_value(spect, 0., 1.)\n","        spect = self.rescale(spect)\n","        return spect\n","\n","normalize_spect = NormSpect()\n","denormalize_spect = DeNormSpect()"],"metadata":{"id":"1s-kCzVLnZhz"},"id":"1s-kCzVLnZhz","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Random Generator input"],"metadata":{"id":"YyrFD1c8nH26"},"id":"YyrFD1c8nH26"},{"cell_type":"code","source":["@tf.function\n","def make_latent_noise(batch_size=1):\n","    g_noises = tf.random.normal((batch_size, params.GEN_NOISE_DIM))\n","    return g_noises\n","\n","@tf.function\n","def make_generator_labels(batch_size=1):\n","    g_labels = [[\n","            tf.random.normal((batch_size, 1), mean=ARO_MEAN, stddev=ARO_STD),\n","            tf.random.normal((batch_size, 1), mean=VAL_MEAN, stddev=VAL_STD),\n","        ],\n","        tf.squeeze(\n","            tf.one_hot(\n","                tf.random.uniform((batch_size, 1), minval=0, maxval=params.RL_HI_BOUND-params.RL_LOW_BOUND, dtype=tf.int32), \n","                params.RL_HI_BOUND-params.RL_LOW_BOUND\n","            ),\n","            axis=1\n","        )\n","    ]\n","    return g_labels\n","\n","@tf.function\n","def make_fake_data(generator, batch_size=1):\n","    g_noises = make_latent_noise(batch_size=batch_size)\n","    g_e_labels, g_r_labels = make_generator_labels(batch_size=batch_size)\n","    out = generator([g_noises, g_e_labels, g_r_labels])\n","    return out, g_e_labels\n","\n","\n","# Uses the generator to generate a single spectrogram\n","def make_example(generator, noise=None, labels=None):\n","    if noise is None:\n","        noise = make_latent_noise()\n","    if labels is None:\n","        labels = make_generator_labels()\n","    out = generator([noise, *labels])\n","    out = denormalize_spect(out)\n","    out = tf.squeeze(out, 0)\n","    return out.numpy()"],"metadata":{"id":"4QnSMVsxnG_s"},"id":"4QnSMVsxnG_s","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Layers for dividing the spectrograms (and labels) into patches for the PatchGAN"],"metadata":{"id":"9hpNrXbApIVM"},"id":"9hpNrXbApIVM"},{"cell_type":"code","source":["class PatchToBatchLayer(layers.Layer):\n","    def __init__(self, **kwargs):\n","        super().__init__(**kwargs)\n","\n","    def call(self, data):\n","        return tf.reshape(data, (-1, params.PATCH_WIDTH, params.MEL_BINS))\n","\n","class PatchToBatchLayer_labels(layers.Layer):\n","    def __init__(self, **kwargs):\n","        super().__init__(**kwargs)\n","\n","    def call(self, data):\n","        n = params.LSTM_WINDOW//params.PATCH_WIDTH\n","        data = tf.repeat(data, n, axis=0, name='repeat_labels_for_all_patches')\n","        return data\n","\n","reshape_to_patch_batch = PatchToBatchLayer()\n","reshape_to_patch_batch_l = PatchToBatchLayer_labels()"],"metadata":{"id":"Hec_QmMcnTuq"},"id":"Hec_QmMcnTuq","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"VVK3bkMXyspd","metadata":{"id":"VVK3bkMXyspd"},"source":["## GAN"]},{"cell_type":"markdown","source":["### Generator losses\n","#### Similarity Loss\n","1. Divide the generated data into two equal length sets\n","2. Calculate MSE between the sets\n","3. Minimise negative MSE $\\implies$ Maximise difference within each pair\n","\n","#### Rhythm Loss\n","1. Take the mean amplitude in the generated spectrogram for each time step\n","2. Perform STFT on the \"mean amplitude\"-signal to extract rhythm frequencies\n","3. Mask all rhythm frequencies except for the label\n","4. Minimise negative rhythm frequency \"amplitude\" $\\implies$ Maximise presence of rhythm"],"metadata":{"id":"LbcnGW56pU4d"},"id":"LbcnGW56pU4d"},{"cell_type":"code","execution_count":null,"id":"HUkhfcO_oq4G","metadata":{"id":"HUkhfcO_oq4G"},"outputs":[],"source":["@tf.function\n","def similarity_loss(data): # pairs the generated data in random pairs and calculates the MSE between them\n","    # data (None, 512, 128) => (None, 65536)\n","    data = tf.reshape(data, (-1, 2, params.LSTM_WINDOW*params.MEL_BINS))\n","    data1, data2 = tf.split(data, 2, axis=1)\n","    \n","    # MSE (pair[0], pair[1])\n","    return -tf.keras.losses.mean_squared_error(data1, data2)\n","\n","@tf.function\n","def rhythm_loss(data, rhythm_labels):\n","    x = tf.reduce_mean(data, axis=2)\n","    x = tf.abs(tf.signal.stft(\n","        x,\n","        params.LSTM_WINDOW,\n","        1,\n","        window_fn=None\n","    ))\n","    x = x[..., params.RL_LOW_BOUND:params.RL_HI_BOUND]\n","    n_rhy = params.RL_HI_BOUND - params.RL_LOW_BOUND\n","    x = tf.squeeze(x, axis=1)\n","    x = tf.math.multiply(x, rhythm_labels) # mask with one-hot labels\n","    return -tf.reduce_max(x, axis=1)"]},{"cell_type":"markdown","source":["### Define GAN connections\n","* Send generator output + input labels to the discriminators\n","* If PatchGAN $\\implies$ Divide spectrograms into patches & distribute labels"],"metadata":{"id":"q0yHsYnLrb0K"},"id":"q0yHsYnLrb0K"},{"cell_type":"code","execution_count":null,"id":"xhQmlZp6i8yd","metadata":{"id":"xhQmlZp6i8yd"},"outputs":[],"source":["def define_gan(generator, discriminator, patch=False):\n","    discriminator.trainable = False\n","    noise_g, e_labels, rhy_label = generator.input\n","    g_out = generator.output\n","    \n","    labels_d = e_labels\n","    if patch:\n","        g_out = PatchToBatchLayer()(g_out)\n","        labels_a, labels_v = labels_d\n","        labels_a = PatchToBatchLayer_labels()(labels_a)\n","        labels_v = PatchToBatchLayer_labels()(labels_v)\n","        labels_d = [labels_a, labels_v]\n","    gan_out = discriminator([g_out, labels_d])\n","\n","    model = tf.keras.Model([noise_g, e_labels, rhy_label], gan_out)\n","    return model"]},{"cell_type":"code","execution_count":null,"id":"rRB3sKzxM0W4","metadata":{"id":"rRB3sKzxM0W4"},"outputs":[],"source":["class GAN(tf.keras.Model):\n","    def __init__(self, generator, discriminator, patch_disc, patch_weight=params.PATCH_WEIGHT, **kwargs):\n","        super(GAN, self).__init__(**kwargs)\n","        self.generator = generator\n","        self.gan = define_gan(generator, discriminator)\n","        self.patchgan = define_gan(generator, patch_disc, patch=True)\n","        self.patch_weight = patch_weight\n","        #self.gan.summary()\n","        \n","        self.loss_tracker = tf.keras.metrics.Mean(name=\"generator loss\")\n","        self.patch_loss_tracker = tf.keras.metrics.Mean(name='patch generator loss')\n","        self.combined_loss_tracker = tf.keras.metrics.Mean(name='combined generator loss')\n","        self.sim_loss_tracker = tf.keras.metrics.Mean(name=\"similarity generator loss\")\n","        self.rhy_loss_tracker = tf.keras.metrics.Mean(name=\"rhythm generator loss\")\n","    \n","    def compile(self, optimizer=None, loss=None, patch_loss=None):\n","        super(GAN, self).compile()\n","        self.opt = optimizer\n","        self.loss = loss\n","        self.patch_loss = patch_loss\n","\n","    @property\n","    def metrics(self):\n","        return [self.loss_tracker, \n","                self.patch_loss_tracker, \n","                self.combined_loss_tracker, \n","                self.sim_loss_tracker, \n","                self.rhy_loss_tracker]\n","    \n","    # trains generator\n","    def train_step(self, data):\n","        g_in = data[0]\n","        _, _, rhythm_labels = g_in\n","\n","        with tf.GradientTape() as tape:\n","            pred = self.gan(g_in)     # predict sequences\n","            labels = tf.ones(tf.shape(pred))\n","            d_loss = self.loss(pred, labels)   # G_loss: D(G(z))\n","            self.loss_tracker.update_state(d_loss)\n","\n","            # patch loss\n","            patch_pred = self.patchgan(g_in)\n","            patch_labels = tf.ones(tf.shape(patch_pred))\n","            patch_loss = self.patch_loss(patch_pred, patch_labels)\n","            self.patch_loss_tracker.update_state(patch_loss)  # log loss before weighting\n","\n","            # similarity loss\n","            g_spect = self.generator(g_in)\n","            sim_loss = similarity_loss(g_spect) * params.SIM_WEIGHT\n","            self.sim_loss_tracker.update_state(sim_loss)\n","\n","            # rhythm loss\n","            rhy_loss = rhythm_loss(g_spect, rhythm_labels) * params.RHYTHM_WEIGHT\n","            self.rhy_loss_tracker.update_state(rhy_loss)\n","            \n","            combined_loss = tf.reduce_mean(d_loss) + \\\n","                            tf.reduce_mean(patch_loss) + \\\n","                            tf.reduce_mean(sim_loss) + \\\n","                            tf.reduce_mean(rhy_loss)\n","            self.combined_loss_tracker.update_state(combined_loss)\n","        \n","        grads = tape.gradient(combined_loss, self.generator.trainable_weights)\n","        self.opt.apply_gradients(zip(grads, self.generator.trainable_weights))\n","\n","        return {\n","            \"g_loss\":self.loss_tracker.result(),\n","            \"g_patch_loss\":self.patch_loss_tracker.result(),\n","            \"g_combined_loss\":self.combined_loss_tracker.result(),\n","            \"g_sim_loss\":self.sim_loss_tracker.result(),\n","            \"g_rhy_loss\":self.rhy_loss_tracker.result()\n","        }"]},{"cell_type":"markdown","id":"g0PWKN57Ao2R","metadata":{"id":"g0PWKN57Ao2R"},"source":["## Training loop\n"]},{"cell_type":"code","execution_count":null,"id":"uXi9RqHkiVzm","metadata":{"id":"uXi9RqHkiVzm"},"outputs":[],"source":["def train(discriminator, patch_disc, gan, dataset, epochs = 10):\n","    batch_size = params.BATCH_SIZE # change this for LSTM\n","\n","    d_loss_tracker = tf.keras.metrics.Mean('d_loss', dtype=tf.float32)\n","    g_loss_tracker = tf.keras.metrics.Mean('g_loss', dtype=tf.float32)\n","\n","    current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n","    d_log_dir = 'logs/train/' + current_time + '/disc'\n","    d_summary_writer = tf.summary.create_file_writer(d_log_dir)\n","    g_log_dir = 'logs/train/' + current_time + '/gen'\n","    g_summary_writer = tf.summary.create_file_writer(g_log_dir)\n","    \n","    config_file = 'models/train/' + current_time + '/params.py'\n","    os.makedirs(os.path.dirname(config_file), exist_ok=True)\n","    !cp params.py $config_file\n","\n","    const_noise = make_latent_noise()\n","    const_label = make_generator_labels()\n","\n","    for epoch in range(epochs):\n","        print(f\"Starting epoch {epoch+1}\")\n","        t_start = time.time()\n","        \n","        data = iter(dataset)\n","\n","        n_batches = int(np.ceil(len(dataset) / params.N_CRITIC))\n","        b_out = display(IPython.display.Pretty(f'Starting...'), display_id=True)\n","        for batch in range(n_batches):\n","            \n","            b_out.update(IPython.display.Pretty(f\"Batch {batch+1} of {n_batches}.\"))\n","            # train discriminator\n","            for _ in range(params.N_CRITIC):\n","                try:\n","                    # get real batch\n","\n","                    (X_real_S, X_real_L) = data.get_next()\n","                    X_real_S = normalize_spect(X_real_S)\n","                    \n","                    # generate batch\n","                    X_fake_S, X_fake_L = make_fake_data(gan.generator, batch_size=X_real_S.shape[0]) # make batch of as many fake as real spectrograms\n","                    \n","                    # create labels (real: 1, fake: -1)\n","                    l_real = tf.ones((X_real_S.shape[0], 1))\n","                    l_fake = -tf.ones((X_fake_S.shape[0], 1))\n","                    \n","                    X = (tf.concat([X_real_S, X_fake_S], 0), (\n","                            tf.concat([X_real_L[0], X_fake_L[0]], 0),\n","                            tf.concat([X_real_L[1], X_fake_L[1]], 0))\n","                    )\n","                    l = tf.concat([l_real, l_fake], 0)\n","\n","                    d_loss = discriminator.train_on_batch(x=X, y=l, reset_metrics=True)\n","\n","                    d_loss_tracker(d_loss)\n","\n","                    # patch discriminator training\n","                    X_real_p = reshape_to_patch_batch(X_real_S)\n","                    X_fake_p = reshape_to_patch_batch(X_fake_S)\n","\n","                    # repeat all labels for all patches of spectrogram\n","                    X_real_L_a = reshape_to_patch_batch_l(X_real_L[0])\n","                    X_real_L_v = reshape_to_patch_batch_l(X_real_L[1])\n","                    X_fake_L_a = reshape_to_patch_batch_l(X_fake_L[0])\n","                    X_fake_L_v = reshape_to_patch_batch_l(X_fake_L[1])\n","\n","                    l_real_p = tf.ones((X_real_p.shape[0], 1))\n","                    l_fake_p = -tf.ones((X_fake_p.shape[0], 1))\n","\n","                    X_p = (tf.concat([X_real_p, X_fake_p], 0), (\n","                            tf.concat([X_real_L_a, X_fake_L_a], 0),\n","                            tf.concat([X_real_L_v, X_fake_L_v], 0))\n","                    )\n","                    l_p = tf.concat([l_real_p, l_fake_p], 0)\n","                    pd_loss = patch_disc.train_on_batch(x=X_p, y=l_p, reset_metrics=True)\n","                \n","                except tf.errors.OutOfRangeError: # if N_CRITIC batches are not available\n","                    break\n","            \n","            tb_batch = batch + epoch*n_batches\n","            # update tensorboard\n","            with d_summary_writer.as_default():\n","                tf.summary.scalar('d_loss', d_loss, step=tb_batch)\n","                tf.summary.scalar('pd_loss', pd_loss, step=tb_batch)\n","\n","            # make noise\n","            g_noises = make_latent_noise(batch_size=batch_size)\n","            g_labels = make_generator_labels(batch_size=batch_size)\n","\n","            # train generator\n","            g_loss = gan.train_on_batch([g_noises, *g_labels], reset_metrics=True, return_dict=True)\n","            \n","            g_loss_tracker(g_loss['g_combined_loss'])\n","            \n","            # update tensorboard logs\n","            with g_summary_writer.as_default():\n","                tf.summary.scalar('g_loss', g_loss['g_loss'], step=tb_batch)\n","                tf.summary.scalar('g_patch_loss', g_loss['g_patch_loss'], step=tb_batch)\n","                tf.summary.scalar('g_combined_loss', g_loss['g_combined_loss'], step=tb_batch)\n","                tf.summary.scalar('g_sim_loss', g_loss['g_sim_loss'], step=tb_batch)\n","                tf.summary.scalar('g_rhy_loss', g_loss['g_rhy_loss'], step=tb_batch)\n","            \n","            print('', end='\\r')\n","        # print metrics\n","        print(f\"\\nEpoch {epoch+1} end ({time.time() - t_start:.1f}s):\")\n","        print(f\"  Discriminator loss: {np.mean(d_loss_tracker.result())}\")\n","        print(f\"  Generator loss: {np.mean(g_loss_tracker.result())}\")\n","        \n","\n","        # reset all metrics for new epoch\n","        d_loss_tracker.reset_states()\n","        g_loss_tracker.reset_states()\n","\n","        # generate example\n","        for noise, labels in zip((const_noise, make_latent_noise()), \n","                                 (const_label, make_generator_labels())):\n","            out = make_example(gan.generator, noise=noise, labels=labels)\n","            \n","            show_spectrogram(out, title=f\"min dB:{np.min(out)}, max dB:{np.max(out)}\")\n","            show_audio(out)\n","    \n","        # save model\n","        models = {'gen':generator, 'disc':discriminator, 'p_disc':patch_disc}\n","        for model in models.keys():\n","            filename = 'models/train/' + current_time + f'/{model}_{epoch+1}.h5'\n","            models[model].save(filename)"]},{"cell_type":"code","execution_count":null,"id":"owqW-V26mdgi","metadata":{"id":"owqW-V26mdgi"},"outputs":[],"source":["# labels are either 1 or -1, depending on if the loss should be minimised or maximised\n","def wasserstein_loss(pred, labels):\n","    return tf.math.multiply(pred, labels)\n","\n","class WassersteinLoss(tf.keras.losses.Loss):\n","    def __init__(self, weight=1.):\n","        super().__init__(name=\"WassersteinLoss\")\n","        self.weight = weight\n","    \n","    def call(self, pred, labels):\n","        return wasserstein_loss(pred, labels) * self.weight"]},{"cell_type":"markdown","id":"0A9s7C3ARzd8","metadata":{"id":"0A9s7C3ARzd8"},"source":["## Set up Models"]},{"cell_type":"code","execution_count":null,"id":"LLnBhWrUQ3AT","metadata":{"id":"LLnBhWrUQ3AT"},"outputs":[],"source":["#continue_training = \"drive/MyDrive/Master Thesis/notebooks/models/train/good_big/{model}_30.h5\"\n","continue_training = None # uncomment to train from scratch\n","\n","if continue_training is not None:\n","    discriminator = tf.keras.models.load_model(\n","        continue_training.format(model = 'disc'), \n","        custom_objects={\"wasserstein_loss\": wasserstein_loss,\n","                        \"similarity_loss\": similarity_loss,\n","                        \"WeightClip\":WeightClip,\n","                        \"WassersteinLoss\":WassersteinLoss}\n","    )\n","    discriminator.trainable = True   \n","    patch_disc = tf.keras.models.load_model(\n","        continue_training.format(model = 'p_disc'), \n","        custom_objects={\"wasserstein_loss\": wasserstein_loss,\n","                        \"similarity_loss\": similarity_loss,\n","                        \"WeightClip\":WeightClip,\n","                        \"WassersteinLoss\":WassersteinLoss}\n","    )\n","    patch_disc.trainable = True\n","    generator = tf.keras.models.load_model(\n","        continue_training.format(model = 'gen'), \n","        custom_objects={\"wasserstein_loss\": wasserstein_loss,\n","                        \"similarity_loss\": similarity_loss,\n","                        \"WeightClip\":WeightClip,\n","                        \"WassersteinLoss\":WassersteinLoss}\n","    )\n","else:\n","    discriminator = create_discriminator()\n","    patch_disc = create_patch_discriminator()\n","    generator = create_generator()\n","\n","discriminator.compile(optimizer=tf.keras.optimizers.RMSprop(params.D_LR), \n","                      loss=WassersteinLoss(weight=params.GAN_WEIGHT))\n","patch_disc.compile(optimizer=tf.keras.optimizers.RMSprop(params.PD_LR), \n","                   loss=WassersteinLoss(weight=params.PATCH_WEIGHT))\n","\n","gan = GAN(generator, discriminator, patch_disc)\n","gan.compile(\n","    optimizer=tf.keras.optimizers.RMSprop(params.G_LR), \n","    loss=WassersteinLoss(weight=params.GAN_WEIGHT), \n","    patch_loss=WassersteinLoss(weight=params.PATCH_WEIGHT)\n",")"]},{"cell_type":"markdown","id":"f6el-9O4Aww9","metadata":{"id":"f6el-9O4Aww9"},"source":["# Training Output"]},{"cell_type":"code","execution_count":null,"id":"H5Ezzh_rQLOD","metadata":{"id":"H5Ezzh_rQLOD"},"outputs":[],"source":["%tensorboard --logdir logs/train"]},{"cell_type":"code","execution_count":null,"id":"Wy7sL_uzRQCP","metadata":{"id":"Wy7sL_uzRQCP"},"outputs":[],"source":["train(discriminator, patch_disc, gan, ds_train, epochs=30)"]},{"cell_type":"markdown","id":"nCnJyfc3lc2-","metadata":{"id":"nCnJyfc3lc2-"},"source":["# Misc."]},{"cell_type":"code","execution_count":null,"id":"NmdamUv2Slvv","metadata":{"id":"NmdamUv2Slvv"},"outputs":[],"source":["drive.mount('/content/drive', force_remount=True)\n","!cp -r ./logs/* \"$path_top/logs/\"\n","!cp -r ./models/* \"$path_top/models/\""]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["1c434783","3bbdd913","3ac8a71a","f6b35a5b","16b44fbf","53b80ac6","gbHC67KDl1F4","57ca1b10","37067b4d","c05a4f11","ek9LPu5TAYMq","iv4lmldTqEde","p-5SPePwQ4Vo","1C4OUW1gAshw","RJSiETI8n2J-","YyrFD1c8nH26","9hpNrXbApIVM","VVK3bkMXyspd","LbcnGW56pU4d","q0yHsYnLrb0K","g0PWKN57Ao2R","0A9s7C3ARzd8","f6el-9O4Aww9"],"provenance":[{"file_id":"1nDuXa-UJXc-MpxUyCvLgAbOOSRFIUMNP","timestamp":1681805116042}],"machine_shape":"hm"},"gpuClass":"premium","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"}},"nbformat":4,"nbformat_minor":5}